{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCP2goG-Zuj"
      },
      "source": [
        "Name : Vinayak Renu Nair\r\n",
        "\r\n",
        "Panther ID : 002553736"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "247u5ndy-WUy"
      },
      "source": [
        "1) Create a cosine similarity matrix for all of Shakespeare’s works found in the provided\r\n",
        "file. This will result in a 42 by 42 matrix with the cosine similarity between each of his\r\n",
        "works. In other words, calculate the document-wise cosine similarity between all of\r\n",
        "Shakepeare’s works. Use TF_IDF for this. Note, you can use the Cosine\r\n",
        "Similarity function on scikit-learn or implement your own, but no other library/package is\r\n",
        "allowed.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbYCQ9PQ4k5B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b03e2a9a-1447-4c6d-d8bd-1f4d68a95269"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "from sklearn.metrics.pairwise import linear_kernel\r\n",
        "from sklearn.datasets import fetch_20newsgroups\r\n",
        "import os, random\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "nltk.download('reuters')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MykIeXWU5BA2"
      },
      "source": [
        "path = '/content/drive/MyDrive/shakespeares-works_TXT_FolgerShakespeare'"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI3LQtiC6cnq"
      },
      "source": [
        "txt = os.listdir(path)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzOBwPQ26i9W"
      },
      "source": [
        "data = []\r\n",
        "\r\n",
        "for f in txt :\r\n",
        "  data.append(open(os.path.join(path,f), 'r').read())"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLe-5Ec-9bDp"
      },
      "source": [
        "tfidf = TfidfVectorizer()\r\n",
        "matrix = tfidf.fit_transform(data)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPExMFSa9xvf"
      },
      "source": [
        "cosine_sim = cosine_similarity(matrix,matrix)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VlGn66-K984C",
        "outputId": "b792ef61-daea-474c-db5c-859c024231c2"
      },
      "source": [
        "print(cosine_sim)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.77494139 0.70637941 ... 0.81097319 0.59536997 0.66267568]\n",
            " [0.77494139 1.         0.69990711 ... 0.83172303 0.59251467 0.65190502]\n",
            " [0.70637941 0.69990711 1.         ... 0.7302251  0.56777768 0.62809744]\n",
            " ...\n",
            " [0.81097319 0.83172303 0.7302251  ... 1.         0.61221239 0.67887912]\n",
            " [0.59536997 0.59251467 0.56777768 ... 0.61221239 1.         0.52581905]\n",
            " [0.66267568 0.65190502 0.62809744 ... 0.67887912 0.52581905 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtCLV9qA-f2A"
      },
      "source": [
        "2) Write a function that takes the previous matrix and a number n as parameters (nothing\r\n",
        "else will be accepted) and return the top n similar works. Use the function to output the\r\n",
        "top 10 similar works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwSDvdZ-9_n1"
      },
      "source": [
        "def similar(cosine_sim, N):\r\n",
        "  for i in range(len(cosine_sim)):\r\n",
        "    cosine_sim[i][i] = 0 # Making diagonal elements as zero, to discard them from sorting algorithm\r\n",
        "\r\n",
        "  index_1d = cosine_sim.flatten().argsort()[-(2*N):]\r\n",
        "  x_idx, y_idx = np.unravel_index(index_1d, cosine_sim.shape)\r\n",
        "  \r\n",
        "  x_idx = np.flip(x_idx)\r\n",
        "  y_idx = np.flip(y_idx)\r\n",
        "\r\n",
        "  similar_works = []\r\n",
        "  for i in range(0,len(x_idx),2):\r\n",
        "    x = x_idx[i]\r\n",
        "    y = y_idx[i]\r\n",
        "    if(x != y):\r\n",
        "      similar_works.append((x,y,cosine_sim[x][y]))\r\n",
        "  \r\n",
        "  return similar_works"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dNpRGT9hsgh"
      },
      "source": [
        "similar_works = similar(cosine_sim,10)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yhk2JAd7hu9B",
        "outputId": "60532512-a6c9-4920-a4e9-69ca57f4c3a7"
      },
      "source": [
        "print('Similar works :')\r\n",
        "for n in similar_works:\r\n",
        "  print(txt[n[0]],' and ',txt[n[1]],' - ',n[2])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similar works :\n",
            "lucrece_TXT_FolgerShakespeare.txt  and  venus-and-adonis_TXT_FolgerShakespeare.txt  -  0.9175430138884014\n",
            "henry-iv-part-2_TXT_FolgerShakespeare.txt  and  henry-iv-part-1_TXT_FolgerShakespeare.txt  -  0.9076165568984115\n",
            "shakespeares-sonnets_TXT_FolgerShakespeare.txt  and  lucrece_TXT_FolgerShakespeare.txt  -  0.867768874887524\n",
            "henry-vi-part-1_TXT_FolgerShakespeare.txt  and  henry-vi-part-2_TXT_FolgerShakespeare.txt  -  0.8437915980298397\n",
            "richard-ii_TXT_FolgerShakespeare.txt  and  richard-iii_TXT_FolgerShakespeare.txt  -  0.8389525010107518\n",
            "shakespeares-sonnets_TXT_FolgerShakespeare.txt  and  venus-and-adonis_TXT_FolgerShakespeare.txt  -  0.8381452108269782\n",
            "henry-v_TXT_FolgerShakespeare.txt  and  henry-viii_TXT_FolgerShakespeare.txt  -  0.8363042342905282\n",
            "henry-vi-part-3_TXT_FolgerShakespeare.txt  and  henry-vi-part-2_TXT_FolgerShakespeare.txt  -  0.8354926670186524\n",
            "henry-vi-part-2_TXT_FolgerShakespeare.txt  and  richard-ii_TXT_FolgerShakespeare.txt  -  0.8343507770652859\n",
            "king-john_TXT_FolgerShakespeare.txt  and  henry-v_TXT_FolgerShakespeare.txt  -  0.8342954531999512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIt210eICcBW"
      },
      "source": [
        "3) Using the code from the Language Models II class, train two simple language models\r\n",
        "using all of the files (together) in shakespeares-works_TXT_FolgerShakespeare.zip. One\r\n",
        "model should be trained using bigrams, the other using trigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV15EN6tEIOj"
      },
      "source": [
        "tokens = []\r\n",
        "for doc in data:\r\n",
        "  tokens.append(nltk.word_tokenize(doc))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvCrgMLfjUDg"
      },
      "source": [
        "Using Bigrams :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXlv6x1YIio2"
      },
      "source": [
        "from nltk.corpus import reuters\r\n",
        "from nltk import bigrams, trigrams\r\n",
        "from collections import Counter, defaultdict"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyleFw63jfQB"
      },
      "source": [
        "# Create a placeholder for model\r\n",
        "model_bigram = defaultdict(lambda: defaultdict(lambda: 0))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S30umK6pjgnr"
      },
      "source": [
        "# Count frequency of co-occurence  \r\n",
        "for sentence in tokens:\r\n",
        "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\r\n",
        "        model_bigram[w1][w2] += 1"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHTN3LS2jggQ"
      },
      "source": [
        "# Let's transform the counts to probabilities\r\n",
        "for w1 in model_bigram:\r\n",
        "    total_count = float(sum(model_bigram[w1].values()))\r\n",
        "    for w2 in model_bigram[w1]:\r\n",
        "        model_bigram[w1][w2] /= total_count"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rIlcB4lUjgY7",
        "outputId": "9f61deef-d359-4d73-8712-44db9a5604c0"
      },
      "source": [
        "dict(model_bigram[\"King\"])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0.011428571428571429,\n",
              " \"''\": 0.0016326530612244899,\n",
              " \"'s\": 0.09551020408163265,\n",
              " ',': 0.11836734693877551,\n",
              " '--': 0.0024489795918367346,\n",
              " '.': 0.08897959183673469,\n",
              " ':': 0.004081632653061225,\n",
              " ';': 0.004081632653061225,\n",
              " '?': 0.0236734693877551,\n",
              " 'And': 0.006530612244897959,\n",
              " 'Be': 0.0008163265306122449,\n",
              " 'Bolingbroke': 0.0008163265306122449,\n",
              " 'Brothers': 0.0008163265306122449,\n",
              " 'Cambyses': 0.0008163265306122449,\n",
              " 'Capaneus': 0.0008163265306122449,\n",
              " 'Cerberus': 0.0008163265306122449,\n",
              " 'Claudius': 0.0008163265306122449,\n",
              " 'Clothair': 0.0008163265306122449,\n",
              " 'Cophetua': 0.0024489795918367346,\n",
              " 'Digest': 0.0008163265306122449,\n",
              " 'Dismiss': 0.0008163265306122449,\n",
              " 'Does': 0.0008163265306122449,\n",
              " 'Duncan': 0.0024489795918367346,\n",
              " 'Edward': 0.027755102040816326,\n",
              " 'Five': 0.0008163265306122449,\n",
              " 'For': 0.0008163265306122449,\n",
              " 'From': 0.0008163265306122449,\n",
              " 'Gorboduc': 0.0008163265306122449,\n",
              " 'Had': 0.0016326530612244899,\n",
              " 'Hal': 0.0008163265306122449,\n",
              " 'Hamlet': 0.0032653061224489797,\n",
              " 'Harry': 0.005714285714285714,\n",
              " 'Hath': 0.0016326530612244899,\n",
              " 'Have': 0.0008163265306122449,\n",
              " 'Henry': 0.07428571428571429,\n",
              " 'I': 0.0024489795918367346,\n",
              " 'In': 0.0008163265306122449,\n",
              " 'Is': 0.0016326530612244899,\n",
              " 'John': 0.029387755102040815,\n",
              " 'Knows': 0.0008163265306122449,\n",
              " 'Lear': 0.0024489795918367346,\n",
              " 'Leontes': 0.0016326530612244899,\n",
              " 'Lewis': 0.006530612244897959,\n",
              " 'Manchus': 0.0008163265306122449,\n",
              " 'Of': 0.0032653061224489797,\n",
              " 'Pepin': 0.0016326530612244899,\n",
              " 'Pericles': 0.004897959183673469,\n",
              " 'Pharamond': 0.0008163265306122449,\n",
              " 'Philip': 0.007346938775510204,\n",
              " 'Pippen': 0.0016326530612244899,\n",
              " 'Polixenes': 0.0008163265306122449,\n",
              " 'Prettily': 0.0008163265306122449,\n",
              " 'Priam': 0.0024489795918367346,\n",
              " 'Reignier': 0.0008163265306122449,\n",
              " 'Richard': 0.026938775510204082,\n",
              " 'Seems': 0.0008163265306122449,\n",
              " 'Shall': 0.0016326530612244899,\n",
              " 'Should': 0.0016326530612244899,\n",
              " 'Simonides': 0.0032653061224489797,\n",
              " 'Smile': 0.0008163265306122449,\n",
              " 'So': 0.0008163265306122449,\n",
              " 'Stephano': 0.0016326530612244899,\n",
              " 'Stephen': 0.0008163265306122449,\n",
              " 'That': 0.0032653061224489797,\n",
              " 'To': 0.0024489795918367346,\n",
              " 'Toward': 0.0008163265306122449,\n",
              " 'Unto': 0.0008163265306122449,\n",
              " 'Urinal': 0.0008163265306122449,\n",
              " 'Yet': 0.0008163265306122449,\n",
              " ']': 0.017142857142857144,\n",
              " 'a': 0.005714285714285714,\n",
              " 'against': 0.0008163265306122449,\n",
              " 'all': 0.0008163265306122449,\n",
              " 'already': 0.0008163265306122449,\n",
              " 'and': 0.04,\n",
              " 'as': 0.0008163265306122449,\n",
              " 'assisted': 0.0008163265306122449,\n",
              " 'at': 0.0008163265306122449,\n",
              " 'back': 0.0008163265306122449,\n",
              " 'before': 0.0016326530612244899,\n",
              " 'being': 0.0008163265306122449,\n",
              " 'besides': 0.0008163265306122449,\n",
              " 'best': 0.0008163265306122449,\n",
              " 'blame': 0.0008163265306122449,\n",
              " 'bow': 0.0008163265306122449,\n",
              " 'but': 0.0008163265306122449,\n",
              " 'by': 0.0024489795918367346,\n",
              " 'call': 0.0016326530612244899,\n",
              " 'calls': 0.0008163265306122449,\n",
              " 'chooses': 0.0008163265306122449,\n",
              " 'come': 0.0024489795918367346,\n",
              " 'comes': 0.0032653061224489797,\n",
              " 'commands': 0.0008163265306122449,\n",
              " 'concerns': 0.0008163265306122449,\n",
              " 'cried': 0.0008163265306122449,\n",
              " 'dead': 0.0016326530612244899,\n",
              " 'did': 0.0016326530612244899,\n",
              " 'dies': 0.0008163265306122449,\n",
              " 'do': 0.0016326530612244899,\n",
              " 'doth': 0.005714285714285714,\n",
              " 'draws': 0.0008163265306122449,\n",
              " 'drinks': 0.0008163265306122449,\n",
              " 'employed': 0.0008163265306122449,\n",
              " 'enacts': 0.0008163265306122449,\n",
              " 'encamped': 0.0008163265306122449,\n",
              " 'entereth': 0.0008163265306122449,\n",
              " 'enters': 0.0016326530612244899,\n",
              " 'escaped': 0.0008163265306122449,\n",
              " 'exceedeth': 0.0008163265306122449,\n",
              " 'exiled': 0.0008163265306122449,\n",
              " 'exit': 0.0024489795918367346,\n",
              " 'exits': 0.004081632653061225,\n",
              " 'falls': 0.0008163265306122449,\n",
              " 'favors': 0.0008163265306122449,\n",
              " 'first': 0.0008163265306122449,\n",
              " 'for': 0.0016326530612244899,\n",
              " 'forbids': 0.0008163265306122449,\n",
              " 'forth': 0.0008163265306122449,\n",
              " 'found': 0.0008163265306122449,\n",
              " 'from': 0.0024489795918367346,\n",
              " 'gone': 0.0016326530612244899,\n",
              " 'grown': 0.0008163265306122449,\n",
              " 'grows': 0.0008163265306122449,\n",
              " 'guilty': 0.0008163265306122449,\n",
              " 'had': 0.0032653061224489797,\n",
              " 'has': 0.004897959183673469,\n",
              " 'hath': 0.02122448979591837,\n",
              " 'have': 0.0016326530612244899,\n",
              " 'he': 0.0016326530612244899,\n",
              " 'hear': 0.0008163265306122449,\n",
              " 'hence': 0.0008163265306122449,\n",
              " 'her': 0.0016326530612244899,\n",
              " 'here': 0.0008163265306122449,\n",
              " 'himself': 0.00816326530612245,\n",
              " 'his': 0.004897959183673469,\n",
              " 'hold': 0.0008163265306122449,\n",
              " 'in': 0.007346938775510204,\n",
              " 'is': 0.031020408163265307,\n",
              " 'keeps': 0.0008163265306122449,\n",
              " 'kisses': 0.0008163265306122449,\n",
              " 'know': 0.0008163265306122449,\n",
              " 'knows': 0.0008163265306122449,\n",
              " 'lack': 0.0008163265306122449,\n",
              " 'languishes': 0.0008163265306122449,\n",
              " 'like': 0.0008163265306122449,\n",
              " 'loves': 0.0008163265306122449,\n",
              " 'made': 0.0008163265306122449,\n",
              " 'may': 0.0008163265306122449,\n",
              " 'mine': 0.0008163265306122449,\n",
              " 'miscarry': 0.0008163265306122449,\n",
              " 'more': 0.0008163265306122449,\n",
              " 'must': 0.0008163265306122449,\n",
              " 'my': 0.013061224489795919,\n",
              " 'nearest': 0.0008163265306122449,\n",
              " 'nor': 0.0008163265306122449,\n",
              " 'not': 0.0008163265306122449,\n",
              " 'now': 0.0008163265306122449,\n",
              " 'of': 0.06612244897959184,\n",
              " 'once': 0.0008163265306122449,\n",
              " 'or': 0.0016326530612244899,\n",
              " 'our': 0.0016326530612244899,\n",
              " 'papers': 0.0008163265306122449,\n",
              " 'permitted': 0.0008163265306122449,\n",
              " 'please': 0.0008163265306122449,\n",
              " 'reads': 0.0008163265306122449,\n",
              " 'recovers': 0.0008163265306122449,\n",
              " 'reposeth': 0.0008163265306122449,\n",
              " 'returned': 0.0008163265306122449,\n",
              " 'returns': 0.0008163265306122449,\n",
              " 'rises': 0.0008163265306122449,\n",
              " 'riseth': 0.0008163265306122449,\n",
              " 'say': 0.0008163265306122449,\n",
              " 'severely': 0.0008163265306122449,\n",
              " 'shall': 0.008979591836734694,\n",
              " 'she': 0.0008163265306122449,\n",
              " 'should': 0.0008163265306122449,\n",
              " 'shows': 0.0008163265306122449,\n",
              " 'so': 0.0008163265306122449,\n",
              " 'speaks': 0.0008163265306122449,\n",
              " 'stands': 0.0008163265306122449,\n",
              " 'step': 0.0008163265306122449,\n",
              " 'steps': 0.0008163265306122449,\n",
              " 'stirring': 0.0008163265306122449,\n",
              " 't': 0.0008163265306122449,\n",
              " 'takes': 0.0016326530612244899,\n",
              " 'taste': 0.0008163265306122449,\n",
              " 'than': 0.0008163265306122449,\n",
              " 'that': 0.006530612244897959,\n",
              " 'the': 0.0008163265306122449,\n",
              " 'this': 0.0016326530612244899,\n",
              " 'to': 0.0032653061224489797,\n",
              " 'today': 0.0008163265306122449,\n",
              " 'tonight': 0.0008163265306122449,\n",
              " 'unhandled': 0.0008163265306122449,\n",
              " 'unto': 0.0008163265306122449,\n",
              " 'very': 0.0008163265306122449,\n",
              " 'was': 0.0032653061224489797,\n",
              " 'were': 0.0008163265306122449,\n",
              " 'whom': 0.0008163265306122449,\n",
              " 'will': 0.009795918367346938,\n",
              " 'wipes': 0.0008163265306122449,\n",
              " 'with': 0.0032653061224489797,\n",
              " 'withal': 0.0008163265306122449,\n",
              " 'would': 0.0016326530612244899,\n",
              " 'your': 0.00816326530612245}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO9votA6jwLK"
      },
      "source": [
        "Using Trigrams :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKxGe6yxjgPH"
      },
      "source": [
        "# Create a placeholder for model\r\n",
        "model_trigram = defaultdict(lambda: defaultdict(lambda: 0))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gplr7a4iM-Pr"
      },
      "source": [
        "# Count frequency of co-occurence  \r\n",
        "for sentence in tokens:\r\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\r\n",
        "        model_trigram[(w1, w2)][w3] += 1"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIK_lY1FKHgD"
      },
      "source": [
        "# Let's transform the counts to probabilities\r\n",
        "for w1_w2 in model_trigram:\r\n",
        "    total_count = float(sum(model_trigram[w1_w2].values()))\r\n",
        "    for w3 in model_trigram[w1_w2]:\r\n",
        "        model_trigram[w1_w2][w3] /= total_count"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zvULgE4aKHew",
        "outputId": "c9cad43c-36db-4341-d7b5-f6a9f3978fe0"
      },
      "source": [
        "dict(model_trigram[\"the\",\"battle\"])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'s\": 0.06666666666666667,\n",
              " ',': 0.23333333333333334,\n",
              " '.': 0.1,\n",
              " 'Which': 0.03333333333333333,\n",
              " 'and': 0.06666666666666667,\n",
              " 'came': 0.03333333333333333,\n",
              " 'ends': 0.03333333333333333,\n",
              " 'fly': 0.03333333333333333,\n",
              " \"ha'\": 0.03333333333333333,\n",
              " 'let': 0.03333333333333333,\n",
              " 'of': 0.03333333333333333,\n",
              " 'range': 0.03333333333333333,\n",
              " 'sought': 0.03333333333333333,\n",
              " 'still': 0.03333333333333333,\n",
              " 'than': 0.03333333333333333,\n",
              " 'think': 0.06666666666666667,\n",
              " 'thus': 0.06666666666666667,\n",
              " 'to': 0.03333333333333333}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4iXPz1TTB6t"
      },
      "source": [
        "4) Write a function that takes the following three parameters: model, list of start words,\r\n",
        "number of sentences to generate. This function should return the sentences generated\r\n",
        "as a list. DO NOT print anything to the screen from within the function. Use this function\r\n",
        "to generate 10 sentences with the bigram model from the previous question, and 5\r\n",
        "sentences with the trigram model from the previous question. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKc2Ks5vLIyC"
      },
      "source": [
        "def new_sentences(model,start_words,num_sentences):\r\n",
        "  sentences = []\r\n",
        "  \r\n",
        "  for key in model.keys():\r\n",
        "    break\r\n",
        "  \r\n",
        "  if(type(key) == tuple):\r\n",
        "    for i in range(num_sentences):\r\n",
        "      text = start_words\r\n",
        "      sentence_finished = False\r\n",
        "\r\n",
        "      while not sentence_finished:\r\n",
        "        # select a random probability threshold  \r\n",
        "        r = random.random()\r\n",
        "        accumulator = .0\r\n",
        "\r\n",
        "        for word in model[tuple(text[-2:])].keys():\r\n",
        "            accumulator += model[tuple(text[-2:])][word]\r\n",
        "            # select words that are above the probability threshold\r\n",
        "            if accumulator >= r:\r\n",
        "                text.append(word)\r\n",
        "                break\r\n",
        "\r\n",
        "        if text[-2:] == [None, None]:\r\n",
        "            sentence_finished = True\r\n",
        "        \r\n",
        "      sentences.append(' '.join([t for t in text if t]))\r\n",
        "  else:\r\n",
        "    for i in range(num_sentences):\r\n",
        "      text = start_words\r\n",
        "      sentence_finished = False\r\n",
        "\r\n",
        "      while not sentence_finished:\r\n",
        "        # select a random probability threshold  \r\n",
        "        r = random.random()\r\n",
        "        accumulator = .0\r\n",
        "\r\n",
        "        for word in model[text[-1]].keys():\r\n",
        "            accumulator += model[text[-1]][word]\r\n",
        "            # select words that are above the probability threshold\r\n",
        "            if accumulator >= r:\r\n",
        "                text.append(word)\r\n",
        "                break\r\n",
        "\r\n",
        "        if text[-1:] == [None]:\r\n",
        "            sentence_finished = True\r\n",
        "        \r\n",
        "      sentences.append(' '.join([t for t in text if t]))\r\n",
        "\r\n",
        "  return sentences"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWh-dsA4LIw3"
      },
      "source": [
        "tri_sentences = new_sentences(model_trigram,['the','battle'],5)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDIKw3VSLIvB"
      },
      "source": [
        "bi_sentences = new_sentences(model_bigram,['battle'],10)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwa92899ntLE"
      },
      "source": [
        "Bonus (20 points): Using the same methodology from questions 1 and 2, create a similarity matrix between the 20 newsgroups corpus. And find the top 5 similar newsgroups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWWV2_7WLItj"
      },
      "source": [
        "newdata = fetch_20newsgroups()"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTWKV2iQnxXH"
      },
      "source": [
        "tfidf = TfidfVectorizer()\r\n",
        "matrix = tfidf.fit_transform(newdata.data)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dRFqHXSanxQ0",
        "outputId": "43e68737-9080-4955-a9d9-0369fa0fcfcd"
      },
      "source": [
        "cosine_sim = cosine_similarity(matrix, matrix)\r\n",
        "print(cosine_sim)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.04405974 0.11017033 ... 0.04433678 0.04457107 0.0329325 ]\n",
            " [0.04405974 1.         0.06242113 ... 0.07373268 0.06959306 0.02439956]\n",
            " [0.11017033 0.06242113 1.         ... 0.07569182 0.06214891 0.02357985]\n",
            " ...\n",
            " [0.04433678 0.07373268 0.07569182 ... 1.         0.02909961 0.00716986]\n",
            " [0.04457107 0.06959306 0.06214891 ... 0.02909961 1.         0.02428174]\n",
            " [0.0329325  0.02439956 0.02357985 ... 0.00716986 0.02428174 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_3yzUy7n2UW"
      },
      "source": [
        "similar_works = similar(cosine_sim,10)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a-K5EvKfn4nO",
        "outputId": "c9e1d868-185e-4936-8f1e-7e2425988ae0"
      },
      "source": [
        "print(similar_works)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(10777, 2002, 1.0000000000000002), (5392, 14, 1.0000000000000002), (9989, 800, 1.0), (4495, 4772, 0.9997809868890166), (8665, 4772, 0.9996809512865921), (981, 9920, 0.999622508768835), (8665, 4495, 0.9995302511028284), (4515, 4495, 0.9992512742959921), (4515, 4772, 0.9990101094182835), (4515, 8665, 0.9989914519713721)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwQX8VSvonBB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}